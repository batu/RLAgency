{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L_wK2B8JJwsQ",
    "outputId": "34529116-8689-4cf3-9467-58da865821ac"
   },
   "source": [
    "# Reinforcement Learning for Navigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "A8tqLUEiNk1_",
    "outputId": "11e8d512-40be-4ebf-9142-7781bf8662b5"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, SAC\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv, MultiAgentVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "\n",
    "\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from gym_unity.envs import UnityToGymWrapper, UnityToMultiGymWrapper\n",
    "import gym_unity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ENV_PATH = r\"C:\\Users\\batua\\Desktop\\RLNav\\NavigationEnvironments\\Debug\\Ball3D\\UnityEnvironment.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _ in range(1024):\n",
    "#     action = np.random.uniform(low=-2, high=2, size=(16,2))\n",
    "#     multi_env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = np.random.uniform(low=-1, high=1, size=(0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"tmp/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "def make_env(rank, seed=0):\n",
    "    def _init():\n",
    "        unity_env = UnityEnvironment(ENV_PATH, base_port=5000 + random.randint(0,1000) + rank)\n",
    "        env = UnityToMultiGymWrapper(unity_env)\n",
    "        env.seed(seed + rank)\n",
    "        env = Monitor(env, log_dir)\n",
    "        return env\n",
    "    set_random_seed(seed)\n",
    "    return _init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-22 19:36:44 INFO [environment.py:113] Connected to Unity environment with package version 1.9.0-preview and communication version 1.5.0\n",
      "2021-04-22 19:36:44 INFO [environment.py:282] Connected new brain:\n",
      "3DBall?team=0\n",
      "2021-04-22 19:36:44 WARNING [__init__.py:106] The environment contains multiple observations. You must define allow_multiple_obs=True to receive them all. Otherwise, only the first visual observation (or vector observation ifthere are no visual observations) will be provided in the observation.\n",
      "C:\\Users\\batua\\anaconda3\\envs\\rlnav\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "2021-04-22 19:36:44 WARNING [__init__.py:336] Could not seed environment 3DBall?team=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "num envs in multiagentvecenv 12\n"
     ]
    }
   ],
   "source": [
    "breaker = random.randint(0,1000)\n",
    "env = MultiAgentVecEnv(make_env(0, breaker), 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.58385758e-02, -2.70688180e-02, -1.43874550e+00,\n",
       "         3.99607611e+00,  9.79341984e-01,  0.00000000e+00,\n",
       "        -1.96199998e-01,  0.00000000e+00],\n",
       "       [ 7.30369315e-02, -7.14064389e-02, -9.66649055e-01,\n",
       "         3.99607611e+00,  1.29474640e-01,  0.00000000e+00,\n",
       "        -1.96199998e-01,  0.00000000e+00],\n",
       "       [ 4.42208499e-02, -7.51109868e-02,  1.27105904e+00,\n",
       "         3.99607563e+00,  9.11190510e-01,  0.00000000e+00,\n",
       "        -1.96199998e-01,  0.00000000e+00],\n",
       "       [ 4.76103649e-02,  2.27974961e-03,  1.14177227e+00,\n",
       "         3.99607611e+00,  3.54968071e-01,  0.00000000e+00,\n",
       "        -1.96199998e-01,  0.00000000e+00],\n",
       "       [ 5.12608923e-02,  2.75232233e-02, -6.70847893e-01,\n",
       "         3.99607563e+00,  1.00081205e+00,  0.00000000e+00,\n",
       "        -1.96199998e-01,  0.00000000e+00],\n",
       "       [-4.58375644e-03,  1.59135635e-03,  1.07326746e-01,\n",
       "         3.99607563e+00, -1.20461130e+00,  0.00000000e+00,\n",
       "        -1.96199998e-01,  0.00000000e+00],\n",
       "       [-1.12987794e-02, -2.17110123e-02,  1.10314941e+00,\n",
       "         3.99607563e+00,  1.14174938e+00,  0.00000000e+00,\n",
       "        -1.96199998e-01,  0.00000000e+00],\n",
       "       [ 3.09453136e-03,  3.12603824e-02,  3.30335617e-01,\n",
       "         3.99607611e+00,  7.02146530e-01,  0.00000000e+00,\n",
       "        -1.96199998e-01,  0.00000000e+00],\n",
       "       [-3.79380770e-02, -6.24894127e-02,  1.45627069e+00,\n",
       "         3.99607611e+00, -9.43095684e-01,  0.00000000e+00,\n",
       "        -1.96199998e-01,  0.00000000e+00],\n",
       "       [ 8.49413723e-02,  2.27954108e-02,  1.13527143e+00,\n",
       "         3.99607611e+00,  8.51564407e-02,  0.00000000e+00,\n",
       "        -1.96199998e-01,  0.00000000e+00],\n",
       "       [ 2.75760330e-02, -2.45826915e-02, -5.69236755e-01,\n",
       "         3.99607611e+00,  6.27917767e-01,  0.00000000e+00,\n",
       "        -1.96199998e-01,  0.00000000e+00],\n",
       "       [-6.25845715e-02, -5.09504825e-02, -9.15328979e-01,\n",
       "         3.99607611e+00,  2.92935371e-01,  0.00000000e+00,\n",
       "        -1.96199998e-01,  0.00000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "for _ in range(128):\n",
    "    action = np.random.uniform(low=-1, high=1, size=(12,2))\n",
    "    env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 39.5         |\n",
      "|    ep_rew_mean          | 3.94         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3128         |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038244352 |\n",
      "|    clip_fraction        | 0.0369       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | 0.0512       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.888        |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00193     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 1.1          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 71.8        |\n",
      "|    ep_rew_mean          | 7.17        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1348        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 36          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006280824 |\n",
      "|    clip_fraction        | 0.0466      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | -0.18       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0216      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00223    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.0629      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 63           |\n",
      "|    ep_rew_mean          | 6.3          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1134         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 64           |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026576824 |\n",
      "|    clip_fraction        | 0.0238       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.86        |\n",
      "|    explained_variance   | 0.0193       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.151        |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00111     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.273        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 133          |\n",
      "|    ep_rew_mean          | 13.3         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1040         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 94           |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039829947 |\n",
      "|    clip_fraction        | 0.0293       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.88        |\n",
      "|    explained_variance   | 0.0356       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.314        |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0012      |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 0.498        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 54.9         |\n",
      "|    ep_rew_mean          | 5.48         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 999          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 122          |\n",
      "|    total_timesteps      | 122880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032793349 |\n",
      "|    clip_fraction        | 0.0313       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.89        |\n",
      "|    explained_variance   | 0.0359       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.356        |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00148     |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 0.78         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 79.2        |\n",
      "|    ep_rew_mean          | 7.92        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 956         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 154         |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004506148 |\n",
      "|    clip_fraction        | 0.0371      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.89       |\n",
      "|    explained_variance   | 0.04        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.321       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00151    |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 0.977       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 209          |\n",
      "|    ep_rew_mean          | 20.9         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 939          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 183          |\n",
      "|    total_timesteps      | 172032       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039685816 |\n",
      "|    clip_fraction        | 0.034        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.88        |\n",
      "|    explained_variance   | 0.0424       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.382        |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00166     |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 1.3          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 103          |\n",
      "|    ep_rew_mean          | 10.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 925          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 212          |\n",
      "|    total_timesteps      | 196608       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058207624 |\n",
      "|    clip_fraction        | 0.0418       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.87        |\n",
      "|    explained_variance   | 0.038        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.536        |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00183     |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 1.4          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 112         |\n",
      "|    ep_rew_mean          | 11.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 912         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 242         |\n",
      "|    total_timesteps      | 221184      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004344398 |\n",
      "|    clip_fraction        | 0.0368      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.87       |\n",
      "|    explained_variance   | 0.0297      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.804       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00166    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 1.42        |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=2)\n",
    "model.learn(total_timesteps=1000000, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "print(obs.shape)\n",
    "for _ in range(1024):\n",
    "    actions = [model.predict(ob) for ob in obs]\n",
    "    print(np.array(actions).shape)\n",
    "    env.step(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "for _ in range(1000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "UnityML",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
